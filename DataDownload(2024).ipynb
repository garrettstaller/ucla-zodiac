{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cefc233-4d01-4e7c-8967-46798e3a9321",
   "metadata": {},
   "source": [
    "## *<font color = maroon> Notebook 1: </font>* Zodiac Data Download - \n",
    "**This script includes the following:** <br> \n",
    "1. **Uploading Data** <br>\n",
    "    <font color = yellow> **a) Upload data by the year** </font> <br>\n",
    "    *Please ensure that you adjusted the below edit block to your computer's <font color = magenta> directory </font>*\n",
    "    - Data is pulled from your local python directory(s) into this notebook\n",
    "        - It is recommended that data is stored in a data directory with subdirectories for respective years.\n",
    "    - All data is compiled into respective arrays: lat, lon, sst, fluorimetry, etc...<p>\n",
    "    \n",
    "    <font color = yellow> **b) One file at a time** </font> <br>\n",
    "    *Please fill the <font color = magenta> filename </font> variable with the full name of your .txt file, **leave empty otherwise***\n",
    "    - Zodiac data is often titled as `KDS_20241116T082913.txt`\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d399c1-a7cc-4efa-87fa-4299b491566d",
   "metadata": {},
   "source": [
    "2. **Data Processing / Figure Generation** <br>\n",
    "    - Data is processed in the following ways:\n",
    "        - Confined within the greater Southern California Bight\n",
    "        - All latitude and longitude coordinates are convereted from degree minutes seconds to degree decimal\n",
    "        **No averaging, processing or any form of analysis techniques used in this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513385e0-858f-4baf-a953-05953256dc12",
   "metadata": {},
   "source": [
    "3. **Saving Data**\n",
    "    - Data is compiled into a dictionary that may be uploaded to other files as a .pkl file: <br>\n",
    "        <font color = yellow> **a) Upload data by the year** </font> <br>\n",
    "        - `track_data`: Each cruise date is an individual key, embedded within are that cruises respective GPS coords and measured values (see below) <p> \n",
    "        \n",
    "        <font color = yellow> **b) One file at a time** </font> <br>\n",
    "        - `cruise_(date)`: Same structure as previous dictionary but only for one cruise <p>\n",
    "    - Structure of dictionary \n",
    "        - 'Date'\n",
    "            - 'location'\n",
    "                - (lon, lat)\n",
    "            - 'SST'\n",
    "            - 'Flu'\n",
    "        \n",
    "<font color = maroon> **Code is organized based on the three sections defined above, with <font color = magenta> parameter </font> definitions below** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f9ce1-f590-4293-9e0c-6c63b01d9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Edit Code --------- #\n",
    "\n",
    "# ONE FILE EDIT - leave blank if doing years #\n",
    "filename = ['placeholder']\n",
    "\n",
    "# ALL FILES EDIT #\n",
    "# Directory to grab data\n",
    "gen_directory = '\\path\\to\\directory'\n",
    "\n",
    "# Years of folders - will be overridden if filename is not empty\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "# Geographic bounds of data \n",
    "lower_lon, upper_lon = -120, -117\n",
    "lower_lat, upper_lat = 32, 35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43874b-1775-4330-a94e-8533fdb862a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Employed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fee7bc-5706-4519-a60e-f11bd8e2a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob # file sorting \n",
    "import numpy as np  # Importing data from txt. format into array, `np.loadtxt`\n",
    "import locale       # Used in data import loop\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "# mapping\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs #importing the cartopy coordinate reference system library\n",
    "import cartopy.feature as cfeature #importing the cartopy library of surface features\n",
    "\n",
    "# personal functions\n",
    "import functions as zf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a1824-6c39-4303-aa6e-3f1940c86f73",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb6783-a068-4123-bd30-15f00c6b9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Grab Filenames --------- #\n",
    "\n",
    "if not len(filename):\n",
    "\n",
    "    SSD_ALL = []\n",
    "\n",
    "    # Append name of data directories via loop \n",
    "    for y in range(len(years)):\n",
    "        year = years[y]\n",
    "        directory_path = f\"{gen_directory}{year}\"\n",
    "        File = glob.glob(directory_path + '/[kds]*')\n",
    "        File.sort() \n",
    "        SSD_ALL = np.append(SSD_ALL, File)\n",
    "\n",
    "    print(f'Number of files uploaded: {len(SSD_ALL)}')\n",
    "\n",
    "    # Denote index in which data changes format\n",
    "    # After 06/01/2017 files are recorded in a different format \n",
    "    # this index is marked for below loop...\n",
    "    for i in range(len(SSD_ALL)):\n",
    "        file_date = SSD_ALL[i][91:99] \n",
    "        if file_date == '20170601':\n",
    "            format_change_index = i\n",
    "            break\n",
    "            \n",
    "else:\n",
    "    SSD_ALL = filename\n",
    "    print(f'File: {SSD_ALL[0]}')\n",
    "    \n",
    "    # Check if this cruise is or after the change in .txt file format following 06/01/2017\n",
    "    if SSD_ALL[0][4:12] >= '20170601':\n",
    "        format_change_index = 1\n",
    "    else:\n",
    "        format_change_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e572a-0a66-4952-92df-29d8888e655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Upload Data --------- #\n",
    "\n",
    "# First define empty arrays to append to consecutively add imported data to\n",
    "latitude = np.array([])\n",
    "longitude = np.array([])\n",
    "dates = np.array([])\n",
    "SST = np.array([])       # Sea Surface Temperature\n",
    "flu = np.array([])       # Fluorometry \n",
    "\n",
    "# if something goes wrong this array will store name of file which failed\n",
    "bad_files = np.array([])\n",
    "\n",
    "# Now do a looped read using\n",
    "for i in range(len(SSD_ALL)): \n",
    "    # There are a few bad files that prevent the loop from running, to avoid this, we can use a try statement\n",
    "    # It will at first try the following code \n",
    "    try: \n",
    "        # Starting june 1st of 2017, txt files change formatting. So to ensure we upload coloumns properly we check if our data\n",
    "        # is before or after the data above\n",
    "        \n",
    "        # Inidividual cruise before will not be true so will default to first option in else conditional which is for ALL\n",
    "        if format_change_index:\n",
    "            date_of_cruise, SST_str, flu_str, long_str, lat_str = np.loadtxt(SSD_ALL[i], dtype = str,\n",
    "                                                                  usecols = (0,8,11,3,4), unpack = True)\n",
    "            cruise_date = np.array([])\n",
    "            for j in range(len(date_of_cruise)):\n",
    "                # We spilce from 0 to 8 as with change in data format, dates format also changed\n",
    "                # date changed to including the full year (2017 versus just 17)\n",
    "                time = date_of_cruise[j][0:8]\n",
    "                cruise_date = np.append(cruise_date, time)\n",
    "        \n",
    "        else:\n",
    "            if i < format_change_index:\n",
    "                date_of_cruise, SST_str, flu_str, long_str, lat_str = np.loadtxt(SSD_ALL[i], dtype = str,\n",
    "                                       usecols = (0,5,8,2,3), unpack = True)\n",
    "                # Let us sort the date, such that it does not include commas\n",
    "                cruise_date = np.array([])\n",
    "                for j in range(len(date_of_cruise)):\n",
    "                    # We splice from 0 to 6 to get the simple year, month, day string \n",
    "                    time = date_of_cruise[j][0:6]\n",
    "                    cruise_date = np.append(cruise_date, time)\n",
    "            else: \n",
    "                date_of_cruise, SST_str, flu_str, long_str, lat_str = np.loadtxt(SSD_ALL[i], dtype = str,\n",
    "                                       usecols = (0,8,11,3,4), unpack = True)\n",
    "                cruise_date = np.array([])\n",
    "                for j in range(len(date_of_cruise)):\n",
    "                    # We spilce from 0 to 8 as with change in data format, dates format also changed\n",
    "                    # date changed to including the full year (2017 versus just 17)\n",
    "                    time = date_of_cruise[j][0:8]\n",
    "                    cruise_date = np.append(cruise_date, time)\n",
    "                    \n",
    "        # selects the respective file from the list of files SSD_2015, then pulls from \n",
    "        # cols to add to respective arrays long_str and lat_str\n",
    "        lat_fixed_array  = []\n",
    "        long_fixed_array = []\n",
    "        SST_fixed_array  = []\n",
    "        flu_fixed_array  = []\n",
    "        # To fix the fact that data is imported as strings with commas, use locale imported above\n",
    "        for k in range(len(long_str)):\n",
    "            locale.setlocale(locale.LC_ALL, '')\n",
    "            lat_fixed = locale.atof(lat_str[k])   # for each value remove comma and make float\n",
    "            long_fixed = locale.atof(long_str[k]) # assigns fixed value to variable\n",
    "            SST_fixed = locale.atof(SST_str[k])\n",
    "            flu_fixed = locale.atof(flu_str[k])\n",
    "            # with the fixed variables add them to a new array\n",
    "            lat_fixed_array = np.append(lat_fixed_array, lat_fixed)\n",
    "            long_fixed_array = np.append(long_fixed_array, long_fixed)\n",
    "            SST_fixed_array = np.append(SST_fixed_array, SST_fixed)\n",
    "            flu_fixed_array = np.append(flu_fixed_array, flu_fixed)\n",
    "            # With the fixed arrays for each file, append to arrays that contains\n",
    "            # lat and long coordinate for all of cruises! \n",
    "        latitude = np.append(latitude, lat_fixed_array)\n",
    "        longitude = np.append(longitude, long_fixed_array)\n",
    "        dates = np.append(dates, cruise_date)\n",
    "        SST = np.append(SST, SST_fixed_array)\n",
    "        flu = np.append(flu, flu_fixed_array)\n",
    "        \n",
    "    # If the file is no good, it will assign that to a variable and add it to an array\n",
    "    except:\n",
    "        bad_files = np.append(bad_files, SSD_ALL[i])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd32b4-f346-4211-b652-05fe0bea2109",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Data Organization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81524e3f-cbdb-44bb-a399-4b6073d378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Adjust Bounds --------- #\n",
    "\n",
    "# We will restrict longitude data within -120 and -117 (SoCal Bight)\n",
    "# Boolean Arugument\n",
    "santa_monica_longitude_range = ((longitude > lower_lon) & (longitude < upper_lon))\n",
    "# Apply this to the longitude data\n",
    "lon_range = longitude[santa_monica_longitude_range]\n",
    "\n",
    "# Before setting up a latitude range, we need to ensure that the lengths of the two arrays remain the same \n",
    "# To do this, we will normalize the latitude data to the longitude data. This will select the ith values of the \n",
    "# lat array that correspond with the ith arrays of the longitude array, such that they are of equal length\n",
    "normalized_lat_range = latitude[santa_monica_longitude_range]\n",
    "\n",
    "# Now slice this data for SoCal Bight parameters \n",
    "santa_monica_latitude_range = ((normalized_lat_range > lower_lat) & (normalized_lat_range < upper_lat))\n",
    "lat_range = normalized_lat_range[santa_monica_latitude_range]\n",
    "\n",
    "# Make sure we get the corresponding dates \n",
    "all_dates = dates[santa_monica_longitude_range]\n",
    "all_SST = SST[santa_monica_longitude_range]\n",
    "all_flu = flu[santa_monica_longitude_range]\n",
    "\n",
    "# Check that this worked\n",
    "len(lat_range) == len(lon_range) == len(all_dates) == len(all_SST) == len(all_flu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5839f-3a9b-4db9-9732-602cc986dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Convert lat/lon coords --------- #\n",
    "\n",
    "if format_change_index:\n",
    "    format_change_index = 0\n",
    "\n",
    "# CONVERSION FOR LATITUDE\n",
    "for j in range(len(lat_range)):\n",
    "    lattitude_position = lat_range[j]\n",
    "    if j < format_change_index:\n",
    "        if lattitude_position < 34:\n",
    "            lattitude_string = str(lattitude_position)\n",
    "            # Now for conversion we must seperate first three indices (degrees and decimal places)\n",
    "            # This takes the decimal vals \n",
    "            lattitude_minute_decimal = lattitude_string[3:-1] + lattitude_string[-1]\n",
    "            # Convert these to an integer to perform operations on them for conversion\n",
    "            lattitude_minute_decimal_int = int(lattitude_minute_decimal)\n",
    "            # Convert to decimals by dividing by 60, as 60 sec = 1 minute\n",
    "            lattitude_degree_converted = lattitude_minute_decimal_int//60 # Do // to eliminate remainder\n",
    "            # convert this to a string to add the first three indices back on, the degree location (ex. 33 or 34) and the '.'\n",
    "            lattitude_degree_string = str(lattitude_degree_converted)\n",
    "            lattitude_decimal_degrees_string = lattitude_string[0:3] + lattitude_degree_string\n",
    "            # Convert to float \n",
    "            lat_deg_dec = float(lattitude_decimal_degrees_string)\n",
    "            # Append to array\n",
    "            lat_range[j] = lat_deg_dec\n",
    "        else:\n",
    "            lat_range[j] = lattitude_position\n",
    "    else:\n",
    "        lat_range[j] = lattitude_position\n",
    "\n",
    "# CONVERSION FOR LONGITUDE\n",
    "for j in range(len(lon_range)):\n",
    "    longitude_position = lon_range[j]\n",
    "    if j < format_change_index:\n",
    "        longitude_string = str(longitude_position)\n",
    "        # Now for conversion we must seperate first three indices (degrees and decimal places)\n",
    "        # This takes the decimal vals \n",
    "        longitude_minute_decimal = longitude_string[5:-1] + longitude_string[-1]\n",
    "        # Convert these to an integer to perform operations on them for conversion\n",
    "        longitude_minute_decimal_int = int(longitude_minute_decimal)\n",
    "        # Convert to decimals by dividing by 60, as 60 sec = 1 minute\n",
    "        longitude_degree_converted = longitude_minute_decimal_int//60 # Do // to eliminate remainder\n",
    "        # convert this to a string to add the first three indices back on, the degree location (ex. 33 or 34) and the '.'\n",
    "        longitude_degree_string = str(longitude_degree_converted)\n",
    "        longitude_decimal_degrees_string = longitude_string[0:5] + longitude_degree_string\n",
    "        # Convert to float \n",
    "        lon_deg_dec = float(longitude_decimal_degrees_string)\n",
    "        # Append to array\n",
    "        lon_range[j] = lon_deg_dec\n",
    "    else: \n",
    "        lon_range[j] = longitude_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb627d-6f8e-423b-90cc-35928e24ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lengths of latitude and longitude arrays:\n",
    "print(f'Lengths of Arrays: {len(lon_range)}')\n",
    "\n",
    "# Check range of lat and lon\n",
    "print(f'Longitude maximums and minimums are: {max(lon_range)}, {min(lon_range)}')\n",
    "print(f'Latitude maximums and minimums are: {max(lat_range)}, {min(lat_range)}')\n",
    "print(f'Temperature max and mins are: {max(all_SST)}, {min(all_SST)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71bf89-2f1f-4bf3-9702-a270070bec5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93416156-3c79-4aec-8b3d-dd0384098c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3a. Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde8d18-9d5c-4086-8817-8e5bb4ea291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is a string generator and will so forth be used to get consecutive dates\n",
    "# First define string arrays to pull from in our loop that will concatenate together dates\n",
    "year_strings = ['15', '16', '17', '2017', '2018', '2019', '2020', '2021', '2022', '2023','2024']\n",
    "#####\n",
    "months_of_year = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] \n",
    "#####\n",
    "days_of_month = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10',\n",
    "                 '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', \n",
    "                 '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "\n",
    "latitudes = np.array([])\n",
    "longitudes = np.array([])\n",
    "\n",
    "actual_dates = np.array([])\n",
    "\n",
    "cruise_bins = {}\n",
    "track = {}\n",
    "\n",
    "# Now start loop\n",
    "# First select a year\n",
    "for y in range(len(year_strings)):\n",
    "    year = year_strings[y]\n",
    "    # With year, get a month\n",
    "    for m in range(len(months_of_year)):\n",
    "        month = months_of_year[m]\n",
    "        # finally get respective day of month of some year\n",
    "        for d in range(len(days_of_month)):\n",
    "            day = days_of_month[d]\n",
    "            # On 06/01/2017 the txt files begun storing the data as year month day, not day month year; these conditionals \n",
    "            # account for said switch \n",
    "            if y < 3:\n",
    "                date = day+month+year\n",
    "            else:\n",
    "                date = year+month+day\n",
    "            # Grab all indices in our data that correspond with our date\n",
    "            date_selector  = (all_dates == date)\n",
    "            respective_lat = lat_range[date_selector]\n",
    "            respective_lon = lon_range[date_selector]\n",
    "            respective_SST = all_SST[date_selector]\n",
    "            resepctive_flu = all_flu[date_selector]\n",
    "           \n",
    "        # If there are suffcient amounts of data for that date, meaning it is a date we actually went out, continue \n",
    "            if respective_lat.size > 0:\n",
    "            \n",
    "                # First add the data into the track dictionary \n",
    "                specific_track = {'location': (respective_lon, respective_lat), 'SST': (respective_SST), 'Flu': (resepctive_flu)}\n",
    "                track[date] = specific_track\n",
    "                # record date\n",
    "                actual_dates = np.append(actual_dates, date)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052c88b-96fe-4b8c-a355-2315bcadeb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3b. *Pickle* Files\n",
    "Pickle files allow us to save data and reupload it in other scripts with minimal effort. For more information, click [here](https://pynative.com/python-save-dictionary-to-file/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c930106-8519-498d-af5d-c3bfaca1673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not len(filename):\n",
    "    with open('track_data.pkl', 'wb') as ztd:\n",
    "        pickle.dump(track, ztd)\n",
    "        print('dictionary saved successfully to file')\n",
    "else:\n",
    "    with open(f'cruise_{SSD_ALL[0][4:12]}.pkl', 'wb') as ztd:\n",
    "        pickle.dump(track, ztd)\n",
    "        print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d29c1-8c00-4615-be11-89d1ead7ad98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adbff12-40b0-4688-98b9-c55099625fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' --- Diagnostics of Upload --- \\nNumber of Files Uploaded: {len(SSD_ALL)} \\nIndividual Cruises: {len(actual_dates)}'+\n",
    "      f'\\nNumber of Corrupted Files: {len(bad_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef718da4-bfd5-40c5-aa49-ad50d392b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
