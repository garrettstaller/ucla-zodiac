{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cefc233-4d01-4e7c-8967-46798e3a9321",
   "metadata": {},
   "source": [
    "## *<font color = maroon> Notebook 1: </font>* LOCAL Zodiac Data Download - \n",
    "**This script includes the following:** <br> \n",
    "1. **Uploading Data** <br>\n",
    "    *Please ensure that you adjusted the below edit block to your computer's <font color = magenta> directory </font>*\n",
    "    - Data is pulled from your local python directory into this notebook\n",
    "        - It is recommended that data is stored in a directory with subdirectories for respective years.\n",
    "    - All data is compiled into respective arrays: lat, lon, sst, fluorimetry, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d399c1-a7cc-4efa-87fa-4299b491566d",
   "metadata": {},
   "source": [
    "2. **Data Processing / Figure Generation** <br>\n",
    "    - Data is processed in the following ways:\n",
    "        - Confined within the greater Santa Monica Bay (**SMB**)\n",
    "        - All latitude and longitude coordinates are convereted from degree minutes seconds to degree decimal\n",
    "        **No averaging or any form of analysis techniques used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513385e0-858f-4baf-a953-05953256dc12",
   "metadata": {},
   "source": [
    "3. **Saving Data**\n",
    "    - Data is compiled into a dictionary that may be updated to other files as a .pkl file:\n",
    "        - `track_data`: Each cruise date is an individual key, embedded within are that cruises respective GPS coords and measured values (see below)\n",
    "    - Structure of dictionary \n",
    "        - 'Date'\n",
    "            - 'location'\n",
    "                - (lon, lat)\n",
    "            - 'SST'\n",
    "            - 'Flu'\n",
    "        \n",
    "<font color = maroon> **Code is organized based on the three sections defined above, with <font color = magenta> parameter </font> definitions below** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f9ce1-f590-4293-9e0c-6c63b01d9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Edit Code --------- #\n",
    "\n",
    "# Directory to grab data\n",
    "gen_directory = '\\path\\to\\directory'\n",
    "\n",
    "# Years of folders \n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "# Geographic bounds of data \n",
    "lower_lon, upper_lon = -120, -117\n",
    "lower_lat, upper_lat = 32, 35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43874b-1775-4330-a94e-8533fdb862a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Emplyed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fee7bc-5706-4519-a60e-f11bd8e2a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob # file sorting \n",
    "import numpy as np  # Importing data from txt. format into array, `np.loadtxt`\n",
    "import locale       # Used in data import loop\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "# mapping\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs #importing the cartopy coordinate reference system library\n",
    "import cartopy.feature as cfeature #importing the cartopy library of surface features\n",
    "\n",
    "# personal functions\n",
    "import zodiacfunctions as zf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a1824-6c39-4303-aa6e-3f1940c86f73",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb6783-a068-4123-bd30-15f00c6b9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Grab Filenames --------- #\n",
    "\n",
    "SSD_ALL = []\n",
    "\n",
    "# Append name of data directories via loop \n",
    "for y in range(len(years)):\n",
    "    year = years[y]\n",
    "    directory_path = f\"{gen_directory}{year}\"\n",
    "    File = glob.glob(directory_path + '/[kds]*')\n",
    "    File.sort() \n",
    "    SSD_ALL = np.append(SSD_ALL, File)\n",
    "    \n",
    "print(f'Number of files uploaded: {len(SSD_ALL)}')\n",
    "\n",
    "# Denote index in which data changes format\n",
    "# After 06/01/2017 files are recorded in a different format \n",
    "# this index is marked for below loop...\n",
    "for i in range(len(SSD_ALL)):\n",
    "    file_date = SSD_ALL[i][91:99] \n",
    "    if file_date == '20170601':\n",
    "        format_change_index = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e572a-0a66-4952-92df-29d8888e655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Upload Data --------- #\n",
    "\n",
    "# First define empty arrays to append to consecutively add imported data to\n",
    "latitude = np.array([])\n",
    "longitude = np.array([])\n",
    "dates = np.array([])\n",
    "SST = np.array([])       # Sea Surface Temperature\n",
    "flu = np.array([])       # Fluorometry \n",
    "\n",
    "# if something goes wrong this array will store name of file which failed\n",
    "bad_files = np.array([])\n",
    "\n",
    "# Now do a looped read using\n",
    "for i in range(len(SSD_ALL)): \n",
    "    # There are a few bad files that prevent the loop from running, to avoid this, we can use a try statement\n",
    "    # It will at first try the following code \n",
    "    try: \n",
    "        # Starting june 1st of 2017, txt files change formatting. So to ensure we upload coloumns properly we check if our data\n",
    "        # is before or after the data above\n",
    "        if i < format_change_index:\n",
    "            date_of_cruise, SST_str, flu_str, long_str, lat_str = np.loadtxt(SSD_ALL[i], dtype = str,\n",
    "                                   usecols = (0,5,8,2,3), unpack = True)\n",
    "            # Let us sort the date, such that it does not include commas\n",
    "            cruise_date = np.array([])\n",
    "            for j in range(len(date_of_cruise)):\n",
    "                # We splice from 0 to 6 to get the simple year, month, day string \n",
    "                time = date_of_cruise[j][0:6]\n",
    "                cruise_date = np.append(cruise_date, time)\n",
    "        else: \n",
    "            date_of_cruise, SST_str, flu_str, long_str, lat_str = np.loadtxt(SSD_ALL[i], dtype = str,\n",
    "                                   usecols = (0,8,11,3,4), unpack = True)\n",
    "            cruise_date = np.array([])\n",
    "            for j in range(len(date_of_cruise)):\n",
    "                # We spilce from 0 to 8 as with change in data format, dates format also changed\n",
    "                # date changed to including the full year (2017 versus just 17)\n",
    "                time = date_of_cruise[j][0:8]\n",
    "                cruise_date = np.append(cruise_date, time)\n",
    "        # selects the respective file from the list of files SSD_2015, then pulls from \n",
    "        # cols to add to respective arrays long_str and lat_str\n",
    "        lat_fixed_array  = []\n",
    "        long_fixed_array = []\n",
    "        SST_fixed_array  = []\n",
    "        flu_fixed_array  = []\n",
    "        # To fix the fact that data is imported as strings with commas, use locale imported above\n",
    "        for k in range(len(long_str)):\n",
    "            locale.setlocale(locale.LC_ALL, '')\n",
    "            lat_fixed = locale.atof(lat_str[k])   # for each value remove comma and make float\n",
    "            long_fixed = locale.atof(long_str[k]) # assigns fixed value to variable\n",
    "            SST_fixed = locale.atof(SST_str[k])\n",
    "            flu_fixed = locale.atof(flu_str[k])\n",
    "            # with the fixed variables add them to a new array\n",
    "            lat_fixed_array = np.append(lat_fixed_array, lat_fixed)\n",
    "            long_fixed_array = np.append(long_fixed_array, long_fixed)\n",
    "            SST_fixed_array = np.append(SST_fixed_array, SST_fixed)\n",
    "            flu_fixed_array = np.append(flu_fixed_array, flu_fixed)\n",
    "            # With the fixed arrays for each file, append to arrays that contains\n",
    "            # lat and long coordinate for all of cruises! \n",
    "        latitude = np.append(latitude, lat_fixed_array)\n",
    "        longitude = np.append(longitude, long_fixed_array)\n",
    "        dates = np.append(dates, cruise_date)\n",
    "        SST = np.append(SST, SST_fixed_array)\n",
    "        flu = np.append(flu, flu_fixed_array)\n",
    "    # If the file is no good, it will assign that to a variable and add it to an array\n",
    "    except:\n",
    "        bad_files = np.append(bad_files, SSD_ALL[i])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd32b4-f346-4211-b652-05fe0bea2109",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Data Organization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81524e3f-cbdb-44bb-a399-4b6073d378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Adjust Bounds --------- #\n",
    "\n",
    "# We will restrict longitude data within -120 and -117 (SoCal Bight)\n",
    "# Boolean Arugument\n",
    "santa_monica_longitude_range = ((longitude > lower_lon) & (longitude < upper_lon))\n",
    "# Apply this to the longitude data\n",
    "lon_range = longitude[santa_monica_longitude_range]\n",
    "\n",
    "# Before setting up a latitude range, we need to ensure that the lengths of the two arrays remain the same \n",
    "# To do this, we will normalize the latitude data to the longitude data. This will select the ith values of the \n",
    "# lat array that correspond with the ith arrays of the longitude array, such that they are of equal length\n",
    "normalized_lat_range = latitude[santa_monica_longitude_range]\n",
    "\n",
    "# Now slice this data for SoCal Bight parameters \n",
    "santa_monica_latitude_range = ((normalized_lat_range > lower_lat) & (normalized_lat_range < upper_lat))\n",
    "lat_range = normalized_lat_range[santa_monica_latitude_range]\n",
    "\n",
    "# Make sure we get the corresponding dates \n",
    "all_dates = dates[santa_monica_longitude_range]\n",
    "all_SST = SST[santa_monica_longitude_range]\n",
    "all_flu = flu[santa_monica_longitude_range]\n",
    "\n",
    "# Check that this worked\n",
    "len(lat_range) == len(lon_range) == len(all_dates) == len(all_SST) == len(all_flu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5839f-3a9b-4db9-9732-602cc986dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Convert lat/lon coords --------- #\n",
    "\n",
    "# CONVERSION FOR LATITUDE\n",
    "for j in range(len(lat_range)):\n",
    "    lattitude_position = lat_range[j]\n",
    "    if j < format_change_index:\n",
    "        if lattitude_position < 34:\n",
    "            lattitude_string = str(lattitude_position)\n",
    "            # Now for conversion we must seperate first three indices (degrees and decimal places)\n",
    "            # This takes the decimal vals \n",
    "            lattitude_minute_decimal = lattitude_string[3:-1] + lattitude_string[-1]\n",
    "            # Convert these to an integer to perform operations on them for conversion\n",
    "            lattitude_minute_decimal_int = int(lattitude_minute_decimal)\n",
    "            # Convert to decimals by dividing by 60, as 60 sec = 1 minute\n",
    "            lattitude_degree_converted = lattitude_minute_decimal_int//60 # Do // to eliminate remainder\n",
    "            # convert this to a string to add the first three indices back on, the degree location (ex. 33 or 34) and the '.'\n",
    "            lattitude_degree_string = str(lattitude_degree_converted)\n",
    "            lattitude_decimal_degrees_string = lattitude_string[0:3] + lattitude_degree_string\n",
    "            # Convert to float \n",
    "            lat_deg_dec = float(lattitude_decimal_degrees_string)\n",
    "            # Append to array\n",
    "            lat_range[j] = lat_deg_dec\n",
    "        else:\n",
    "            lat_range[j] = lattitude_position\n",
    "    else:\n",
    "        lat_range[j] = lattitude_position\n",
    "\n",
    "# CONVERSION FOR LONGITUDE\n",
    "for j in range(len(lon_range)):\n",
    "    longitude_position = lon_range[j]\n",
    "    if j < format_change_index:\n",
    "        longitude_string = str(longitude_position)\n",
    "        # Now for conversion we must seperate first three indices (degrees and decimal places)\n",
    "        # This takes the decimal vals \n",
    "        longitude_minute_decimal = longitude_string[5:-1] + longitude_string[-1]\n",
    "        # Convert these to an integer to perform operations on them for conversion\n",
    "        longitude_minute_decimal_int = int(longitude_minute_decimal)\n",
    "        # Convert to decimals by dividing by 60, as 60 sec = 1 minute\n",
    "        longitude_degree_converted = longitude_minute_decimal_int//60 # Do // to eliminate remainder\n",
    "        # convert this to a string to add the first three indices back on, the degree location (ex. 33 or 34) and the '.'\n",
    "        longitude_degree_string = str(longitude_degree_converted)\n",
    "        longitude_decimal_degrees_string = longitude_string[0:5] + longitude_degree_string\n",
    "        # Convert to float \n",
    "        lon_deg_dec = float(longitude_decimal_degrees_string)\n",
    "        # Append to array\n",
    "        lon_range[j] = lon_deg_dec\n",
    "    else: \n",
    "        lon_range[j] = longitude_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb627d-6f8e-423b-90cc-35928e24ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lengths of latitude and longitude arrays:\n",
    "print(f'The lengths of lon and lat are: {len(lon_range)}')\n",
    "\n",
    "# Check range of lat and lon\n",
    "print(f'Longitude maximums and minimums are: {max(lon_range)}, {min(lon_range)}')\n",
    "print(f'Latitude maximums and minimums are: {max(lat_range)}, {min(lat_range)}')\n",
    "print(f'Temperature max and mins are: {max(all_SST)}, {min(all_SST)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cca6f-a091-4d27-a53b-bae880c6a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Figure 1 --------- #\n",
    "\n",
    "# Figure and variable assesment\n",
    "fig1 = plt.figure(1, (14,12))\n",
    "fig1.patch.set_facecolor('white')\n",
    "long_lat_coords = [-118.85, -118.25, 33.55, 34.15]  \n",
    "\n",
    "# Set up map/projection and dimensions\n",
    "ax = plt.axes(projection=ccrs.PlateCarree()) \n",
    "ax.set_extent(long_lat_coords) # Will restrict map to above coordinate domain\n",
    "ax.coastlines()\n",
    "\n",
    "lat_min = 33.55\n",
    "lat_max = 34.15\n",
    "lon_min = -118.85\n",
    "lon_max = -118.25\n",
    "\n",
    "# Split this range into 50 bins, use 51 as linspace is non-inclusive\n",
    "lat = np.linspace(lat_min, lat_max, 51)\n",
    "lon = np.linspace(lon_min, lon_max, 51)\n",
    "\n",
    "### CHECK that respective bin sizes for lon and latitude are ~50 ###\n",
    "lon_size = ((max(lon) - min(lon))/(lon[1]-lon[0]))\n",
    "lat_size = ((max(lat) - min(lat))/(lat[1]-lat[0]))\n",
    "\n",
    "bins = 51\n",
    "\n",
    "\n",
    "# Plot \n",
    "plt.scatter(lon_range, lat_range, s = .01, color = 'dimgrey')\n",
    "\n",
    "# Add grid\n",
    "zf.meshed_gird(lat_min, lat_max, lon_min, lon_max, bins)\n",
    "\n",
    "# Add details to plot\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "\n",
    "# # Notable Features \n",
    "plt.plot(-118.446948, 33.980090,'r*', markerfacecolor = 'none', markersize = 30, label = 'Marina Del Rey')\n",
    "plt.plot(-118.550683, 33.936375,'y*', markerfacecolor = 'none', markersize = 30, label = 'Santa Monica Canyon')\n",
    "plt.plot(-118.442027, 33.828203,'k*', markerfacecolor = 'none', markersize = 30, label = 'Redondo Canyon')\n",
    "\n",
    "# Add some labels \n",
    "plt.suptitle('(2a) All Recorded Zodiac Cruises (2015-2023)', fontsize = 25)\n",
    "ax.set_xticks(np.linspace(-118.85, -118.25, 5),crs=ccrs.PlateCarree()) #set x-ticks\n",
    "ax.set_yticks(np.linspace(33.55, 34.15, 5),crs=ccrs.PlateCarree()) #set y-ticks \n",
    "plt.xlabel('Longitude', fontsize = 20)\n",
    "plt.ylabel('Latitude', fontsize = 20)\n",
    "plt.legend(fontsize = 20, loc = 'upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"ZodiacTracks.png\", dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c77102-8737-4ecb-a7a3-b1fb5da10bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Count Bins --------- #\n",
    "\n",
    "# This loop will count how many times the zodiac enters a bin, only counting a specific bin once per day to avoid\n",
    "# conflation of zodiac presence in predicted high frequency areas, harbor mouth\n",
    "\n",
    "# This code is a string generator and will so forth be used to get consecutive dates\n",
    "# First define string arrays to pull from in our loop that will concatenate together dates\n",
    "year_strings = ['15', '16', '17', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "#####\n",
    "months_of_year = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] \n",
    "#####\n",
    "days_of_month = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10',\n",
    "                 '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', \n",
    "                 '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "\n",
    "latitudes = np.array([])\n",
    "longitudes = np.array([])\n",
    "\n",
    "actual_dates = np.array([])\n",
    "\n",
    "# Now start loop\n",
    "# First select a year\n",
    "for y in range(len(year_strings)):\n",
    "    year = year_strings[y]\n",
    "    # With year, get a month\n",
    "    for m in range(len(months_of_year)):\n",
    "        month = months_of_year[m]\n",
    "        # finally get respective day of month of some year\n",
    "        for d in range(len(days_of_month)):\n",
    "            day = days_of_month[d]\n",
    "            # In 2017 the txt files begun storing the data as year month day, not day month year; these conditionals \n",
    "            # account for said switch \n",
    "            if y < 3:\n",
    "                date = day+month+year\n",
    "            else:\n",
    "                date = year+month+day\n",
    "            # Grab all indices in our data that correspond with our date\n",
    "            date_selector = (all_dates == date)\n",
    "            needed_date = all_dates[date_selector]\n",
    "            respective_lat = lat_range[date_selector]\n",
    "            respective_lon = lon_range[date_selector]\n",
    "            # If there are suffcient amounts of data for that date, meaning it is a date we actually went out, continue \n",
    "            if respective_lat.size > 0:\n",
    "                actual_dates = np.append(actual_dates, date)\n",
    "                # This will now run through the lat array that splits our data into 100 bins \n",
    "                for i in range(len(lat) - 1): \n",
    "                    # Latitude Limit\n",
    "                    lower_lim_y = lat[i] \n",
    "                    upper_lim_y = lat[i + 1]\n",
    "                    # With our limits select all data that fits into the row \n",
    "                    row = (respective_lat >= lower_lim_y) & (respective_lat < upper_lim_y)\n",
    "                    lat_in_row = respective_lat[row]\n",
    "                    # Grab corresponding longitude data\n",
    "                    lon_in_row = respective_lon[row]\n",
    "                    # Only if there are multiple points that the zodiac was in this row can we continue\n",
    "                    if (len(lat_in_row) > 1):\n",
    "                        # Now run through each longitude or coloumn in our row \n",
    "                        for j in range(len(lon) - 1):\n",
    "                            lower_lim_x = lon[j]\n",
    "                            upper_lim_x = lon[j + 1]\n",
    "                            test = (lon_in_row >= lower_lim_x) & (lon_in_row < upper_lim_x)\n",
    "                            long = lon_in_row[test]\n",
    "                            # If there are plenty of longitude points, then we mark this bin \n",
    "                            if (len(long) > 1):\n",
    "                                # We mark the bin by grabbing the mid point of the respective bin \n",
    "                                point_lat = .5*(lower_lim_y + upper_lim_y)\n",
    "                                point_lon = .5*(lower_lim_x + upper_lim_x)\n",
    "                                # Add said bin to an array \n",
    "                                latitudes = np.append(latitudes, point_lat)\n",
    "                                longitudes = np.append(longitudes, point_lon)\n",
    "\n",
    "print(longitudes, latitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc9f97-9abb-4add-a088-bc8f0e813e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Figure 2 --------- #\n",
    "\n",
    "fig2 = plt.figure(1, (14,12))\n",
    "fig2.patch.set_facecolor('white')\n",
    "long_lat_coords = [-118.85, -118.25, 33.55, 34.15]  \n",
    "\n",
    "# Set up map/projection and dimensions\n",
    "ax = plt.axes(projection=ccrs.PlateCarree()) \n",
    "ax.set_extent(long_lat_coords) # Will restrict map to above coordinate domain\n",
    "ax.coastlines()\n",
    "\n",
    "# Plot Histogram \n",
    "plt.hist2d(longitudes, latitudes, bins=(50,50),\n",
    "           range = [(-118.85, -118.25), (33.55, 34.15)],\n",
    "           cmap=cm.ocean, cmin = 1, vmin = 1, vmax = 20)\n",
    "plt.colorbar(shrink = .93, ticks = np.arange(1, 21, 5)).set_label('Number of Trips', fontsize = 20)\n",
    "\n",
    "# Add grid\n",
    "zf.meshed_gird(lat_min, lat_max, lon_min, lon_max, bins)\n",
    "\n",
    "# Add details to plot\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "\n",
    "# Add some labels \n",
    "plt.suptitle('(2b) Frequency of Zodiac Position in $1.5 km^2$ Bins (2015-2023)', fontsize = 25)\n",
    "ax.set_xticks(np.linspace(-118.85, -118.25, 5),crs=ccrs.PlateCarree()) #set x-ticks\n",
    "ax.set_yticks(np.linspace(33.55, 34.15, 5),crs=ccrs.PlateCarree()) #set y-ticks \n",
    "plt.xlabel('Longitude', fontsize = 20)\n",
    "plt.ylabel('Latitude', fontsize = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"ZodiacPositionFrequency.png\", dpi = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71bf89-2f1f-4bf3-9702-a270070bec5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93416156-3c79-4aec-8b3d-dd0384098c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3a. Dictionaries\n",
    "The below script will take the above arrays and sort them into a dictionary:\n",
    "- `Track` = A dictionary that you can use `track_selector` function with parameters 'date_of_cruise' to get all of the lat., lon., and SST arrays for a specific cruise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde8d18-9d5c-4086-8817-8e5bb4ea291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is a string generator and will so forth be used to get consecutive dates\n",
    "# First define string arrays to pull from in our loop that will concatenate together dates\n",
    "year_strings = ['15', '16', '17', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "#####\n",
    "months_of_year = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'] \n",
    "#####\n",
    "days_of_month = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10',\n",
    "                 '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', \n",
    "                 '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "\n",
    "latitudes = np.array([])\n",
    "longitudes = np.array([])\n",
    "\n",
    "actual_dates = np.array([])\n",
    "\n",
    "cruise_bins = {}\n",
    "track = {}\n",
    "\n",
    "# Now start loop\n",
    "# First select a year\n",
    "for y in range(len(year_strings)):\n",
    "    year = year_strings[y]\n",
    "    # With year, get a month\n",
    "    for m in range(len(months_of_year)):\n",
    "        month = months_of_year[m]\n",
    "        # finally get respective day of month of some year\n",
    "        for d in range(len(days_of_month)):\n",
    "            day = days_of_month[d]\n",
    "            # On 06/01/2017 the txt files begun storing the data as year month day, not day month year; these conditionals \n",
    "            # account for said switch \n",
    "            if y < 3:\n",
    "                date = day+month+year\n",
    "            else:\n",
    "                date = year+month+day\n",
    "            # Grab all indices in our data that correspond with our date\n",
    "            date_selector  = (all_dates == date)\n",
    "            respective_lat = lat_range[date_selector]\n",
    "            respective_lon = lon_range[date_selector]\n",
    "            respective_SST = all_SST[date_selector]\n",
    "            resepctive_flu = all_flu[date_selector]\n",
    "            # If there are suffcient amounts of data for that date, meaning it is a date we actually went out, continue \n",
    "            if respective_lat.size > 0:\n",
    "                # First add the data into the track dictionary \n",
    "                specific_track = {'location': (respective_lon, respective_lat), 'SST': (respective_SST), 'Flu': (resepctive_flu)}\n",
    "                track[date] = specific_track\n",
    "                # record date\n",
    "                actual_dates = np.append(actual_dates, date)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052c88b-96fe-4b8c-a355-2315bcadeb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3b. *Pickle* Files\n",
    "Pickle files allow us to save data and reupload it in other scripts with minimal effort. For more information, click [here](https://pynative.com/python-save-dictionary-to-file/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c930106-8519-498d-af5d-c3bfaca1673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save `track` dictionary to zodiac_track_data.pkl file\n",
    "with open('track_data.pkl', 'wb') as ztd:\n",
    "    pickle.dump(track, ztd)\n",
    "    print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d29c1-8c00-4615-be11-89d1ead7ad98",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adbff12-40b0-4688-98b9-c55099625fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' --- Diagnostics of Upload --- \\nNumber of Files Uploaded: {len(SSD_ALL)} \\nIndividual Cruises: {len(actual_dates)}'+\n",
    "      f'\\nNumber of Corrupted Files: {len(bad_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5516a3-a894-4dc5-b254-128a61af3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeaa78-658b-4bd5-ba0b-c3fa30c7df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a080c9d-7b2f-4828-be6c-ee7828d2f07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
